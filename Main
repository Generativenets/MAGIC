import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.utils import to_dense_adj, add_self_loops
import numpy as np
from torch_geometric.nn import GCNConv
from torch_geometric.nn import SAGEConv
from torch_geometric.datasets import Amazon

# loading dataset
dataset = Planetoid(root='/tmp/Cora', name='Cora')
#dataset = Planetoid(root='/tmp/Citeseer', name='Citeseer')
#dataset = Planetoid(root='/tmp/Pubmed', name='Pubmed')
#dataset = Amazon(root='/tmp/Amazon', name='Computers')
#dataset = Amazon(root='/tmp/Amazon', name='Photo')
data = dataset[0]

# divide the dataset 
n = int(data.x.size(0) * 0.6)
k = int(n * 0.6)
all_nodes = np.arange(data.num_nodes)
np.random.shuffle(all_nodes)
masked_nodes = torch.tensor(all_nodes[:n], dtype=torch.long)
train_masked_nodes = masked_nodes[:k]
test_masked_nodes = masked_nodes[k:]

# mask selected nodes
train_mask = torch.ones(data.num_nodes, dtype=bool)
train_mask[masked_nodes] = False
data.train_mask = train_mask

# 移除与masked nodes相关的边并重新映射索引
def remap_index(edge_index, mask):
    mask = mask.nonzero(as_tuple=False).squeeze()
    remapped_dict = {old_idx: new_idx for new_idx, old_idx in enumerate(mask.tolist())}

    row, col = edge_index
    row_remap = torch.tensor([remapped_dict.get(x.item(), -1) for x in row])
    col_remap = torch.tensor([remapped_dict.get(x.item(), -1) for x in col])
    mask = (row_remap != -1) & (col_remap != -1)
    edge_index = torch.stack([row_remap[mask], col_remap[mask]], dim=0)
    return edge_index

train_edge_index = remap_index(data.edge_index, data.train_mask)

# defining the graph learning model on the original graph, for instance, GCN
class GCN(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_features, 16)
        self.conv2 = GCNConv(16, num_classes)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return x



# training the graph learning model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN(dataset.num_node_features, dataset.num_classes).to(device)
data = data.to(device)
model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
for epoch in range(300):
    optimizer.zero_grad()
    out = model(data.x[data.train_mask], train_edge_index)
    loss = F.nll_loss(F.log_softmax(out, dim=1), data.y[data.train_mask])
    loss.backward()
    optimizer.step()

# derive embeddings on the original graph
model.eval()
with torch.no_grad():
    embeddings = model(data.x[data.train_mask], train_edge_index)



class Predictor(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(Predictor, self).__init__()
        self.f1 = torch.nn.Linear(num_features, 16)
        self.f2 = torch.nn.Linear(16, num_classes)


    def forward(self, x):
        x = F.relu(self.f1(x))
        x = F.relu(self.f2(x))
        return F.log_softmax(x, dim=1)

predictor = Predictor(dataset.num_node_features + dataset.num_classes, dataset.num_classes).to(device)

# preparing data for adaptation
edge_index, _ = add_self_loops(data.edge_index)
adj = to_dense_adj(edge_index)[0].to(device)


train_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(data.train_mask.nonzero())}

train_embeddings = []
train_labels = []
for node in train_masked_nodes:
    neighbors = adj[node] == 1
    neighbor_train_nodes = torch.tensor([train_node_mapping.get(n.item(), -1) for n in neighbors.nonzero()]).to(device)
    neighbor_train_nodes = neighbor_train_nodes[neighbor_train_nodes != -1]
    if len(neighbor_train_nodes) > 0:
        aggregated_embedding = embeddings[neighbor_train_nodes].mean(dim=0)
    else:
        aggregated_embedding = torch.zeros_like(embeddings[0])
    node_embedding = torch.cat((aggregated_embedding, data.x[node]), dim=0)
    train_embeddings.append(node_embedding)
    train_labels.append(data.y[node])

train_embeddings = torch.stack(train_embeddings)
train_labels = torch.stack(train_labels)

# Adapatation
predictor_optimizer = torch.optim.Adam(predictor.parameters(), lr=0.01, weight_decay=5e-4)
predictor.train()
for epoch in range(50):
    predictor_optimizer.zero_grad()
    out = predictor(train_embeddings)
    loss = F.nll_loss(out, train_labels)
    loss.backward()
    predictor_optimizer.step()

# predicting nodes on the updated graph
predictor.eval()
test_embeddings = []
test_labels = data.y[test_masked_nodes].cpu().numpy()
with torch.no_grad():
    for node in test_masked_nodes:
        neighbors = adj[node] == 1
        neighbor_train_nodes = torch.tensor([train_node_mapping.get(n.item(), -1) for n in neighbors.nonzero()]).to(device)
        neighbor_train_nodes = neighbor_train_nodes[neighbor_train_nodes != -1]
        if len(neighbor_train_nodes) > 0:
            aggregated_embedding = embeddings[neighbor_train_nodes].mean(dim=0)
        else:
            aggregated_embedding = torch.zeros_like(embeddings[0])
        node_embedding = torch.cat((aggregated_embedding, data.x[node]), dim=0)
        test_embeddings.append(node_embedding)

    test_embeddings = torch.stack(test_embeddings)
    predictions = predictor(test_embeddings).max(1)[1].cpu().numpy()

# predicting accuracy
correct = (predictions == test_labels).sum()
accuracy = correct / len(test_masked_nodes)
print("Prediction accuracy: {:.1f}%".format(100 * accuracy))



